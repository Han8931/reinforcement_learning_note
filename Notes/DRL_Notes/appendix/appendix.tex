\renewcommand{\thesection}{\Alph{section}.\arabic{section}}
\setcounter{section}{0}

\begin{appendices}
\chapter{Appendix}

\section{Markov Process}
\label{appendix:markov_process}
A Markov Process can be defined as follows:
\begin{itemize}
  \item A finite set of states, denoted by \( \mathcal{S} = \{ s_1, s_2, \dots, s_n \} \).
  \item A transition probability matrix \( P \), where each entry \( P(i \to j) \) represents:
	  \begin{align*}
			P(s_{t+1} = s_j \mid s_t = s_i).
	  \end{align*}
\end{itemize}

\begin{definition}[Markov Property]
	A state $S_t$ is \textbf{Markov} if and only if 
	$$P[S_{t+1}|S_t, A_t] = P[S_{t+1}|S_t, A_t, S_{t-1},A_{t-1},...]$$
\end{definition}
\begin{itemize}
	\item Actions: a mechanism to influence the environment
	\item State: specific configurations of the environment
\end{itemize}


\paragraph{Example: A Simple Weather Model}

Consider a system with three possible weather states:
\[
\mathcal{S} = \{ S, C, R \},
\]
where
\begin{itemize}
  \item \(S\) = Sunny,
  \item \(C\) = Cloudy,
  \item \(R\) = Rainy.
\end{itemize}

\subsection{Transition Probabilities}

We define the transition probability matrix \(P\) for moving from one day's weather to the next. 
Let us list the rows in the order \(\{S, C, R\}\). 
Each row of \(P\) corresponds to the probabilities of transitioning \textbf{from} a specific current state \textbf{to} each of the possible next states:

\[
P 
\;=\;
\begin{bmatrix}
P(S \to S) & P(S \to C) & P(S \to R) \\
P(C \to S) & P(C \to C) & P(C \to R) \\
P(R \to S) & P(R \to C) & P(R \to R)
\end{bmatrix}.
\]

Assume:
\[
P(S \to S) = 0.6, \quad 
P(S \to C) = 0.3, \quad 
P(S \to R) = 0.1,
\]
\[
P(C \to S) = 0.4, \quad 
P(C \to C) = 0.2, \quad 
P(C \to R) = 0.4,
\]
\[
P(R \to S) = 0.3, \quad 
P(R \to C) = 0.3, \quad 
P(R \to R) = 0.4.
\]

Hence,
\[
P 
\;=\;
\begin{bmatrix}
0.6 & 0.3 & 0.1 \\[6pt]
0.4 & 0.2 & 0.4 \\[6pt]
0.3 & 0.3 & 0.4
\end{bmatrix}.
\]

Notice each row sums to 1:
\begin{align*}
  0.6 + 0.3 + 0.1 &= 1.0, \\
  0.4 + 0.2 + 0.4 &= 1.0, \\
  0.3 + 0.3 + 0.4 &= 1.0.
\end{align*}
This ensures that from any given state, the probabilities of moving to some next state total 1.

\subsection{Evolution Over Time}

We often track a \textit{probability distribution} over the states at each time step. 
Let \(\pi_t = [\, \pi_t(S),\; \pi_t(C),\; \pi_t(R) \,]\) be a row vector such that:
\[
\pi_t(S) = P\bigl(\text{state at time }t = S\bigr), \quad
\pi_t(C) = P\bigl(\text{state at time }t = C\bigr), \quad
\pi_t(R) = P\bigl(\text{state at time }t = R\bigr).
\]

The \textbf{standard formula} for updating the distribution is:
\[
\pi_{t+1} 
\;=\; 
\pi_t \, P.
\]
This means you multiply your current \textit{row vector} by the transition matrix on the right.

\begin{itemize}
  \item If we know \textbf{Day 0} is \textit{definitely} Sunny, we have:
  \[
    \pi_0 = [\, 1,\; 0,\; 0 \,].
  \]

  \item Then, the distribution for \textbf{Day 1} is:
  \[
    \pi_1 
    = \pi_0 \, P
    = [\, 1,\; 0,\; 0 \,]
      \begin{bmatrix}
      0.6 & 0.3 & 0.1 \\
      0.4 & 0.2 & 0.4 \\
      0.3 & 0.3 & 0.4
      \end{bmatrix}.
  \]
  This multiplication gives:
  \[
    \pi_1 
    = [\, 1 \times 0.6 + 0 \times 0.4 + 0 \times 0.3,\;\;
         1 \times 0.3 + 0 \times 0.2 + 0 \times 0.3,\;\;
         1 \times 0.1 + 0 \times 0.4 + 0 \times 0.4 \,]
    = [\, 0.6,\; 0.3,\; 0.1 \,].
  \]
  Thus, on Day~1:
  \[
    \pi_1(S) = 0.6, \quad \pi_1(C) = 0.3, \quad \pi_1(R) = 0.1,
  \]
  which sums to \(1.0\). This means there is a 60\% chance of Sunny, 30\% chance of Cloudy, and 10\% chance of Rainy for Day~1.
\end{itemize}

In practice, this process continues step by step:
\[
\pi_{t+1} = \pi_t \, P, 
\]
telling you the probability distribution over \(\{S, C, R\}\) on each subsequent day.

\subsection{Why Is This ``Markov''?}
It is called a Markov Process because the probability of the next state (Day~\((t+1)\) weather) depends \textit{only} on the current state (Day~\(t\) weather). It does \textbf{not} depend on the entire history of how the weather evolved prior to Day~\(t\). This property is often referred to as \textit{memorylessness}.

\subsection{Key Takeaways}
\begin{itemize}
  \item A \textbf{finite Markov Process} has a finite set of states and a transition matrix with row sums equal to 1.
  \item The \textit{Markov property} means: 
  \[
   P(S_{t+1} = s_j \;\vert\; S_t = s_i, S_{t-1} = s_{t-1}, \dots) 
   \;=\;
   P(S_{t+1} = s_j \;\vert\; S_t = s_i).
  \]
  \item You can track the evolution of the process over time by repeatedly multiplying the current \textit{distribution} by the transition matrix:
  \[
   \pi_{t+1} = \pi_t \, P.
  \]
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To help conceptualize Finite Markov Processes, let us consider a simple example of changes in inventory at a store. Assume you are the store manager and that your task is to manage the inventory. Let us focus on the inventory of a particular type of bicycle. Assume that each day there is random (non-negative integer) demand for the bicycle with the probabilities of demand following a Poisson distribution (with Poisson parameter $\lambda \in \mathbb{R}_{\geq 0}$), \ie demand $i$ with probability
$$f(i) = \frac {e^{-\lambda} \lambda^i} {i!}$$
Denote $F: \mathbb{Z}_{\geq 0} \rightarrow [0, 1]$ as the Poisson cumulative probability distribution function, i.e.,
 $$F(i) = \sum_{j=0}^i f(j)$$
 \begin{itemize}
	 \item Assume you have storage capacity for at most $C \in \mathbb{Z}_{\geq 0}$ bicycles in your store. 
	 \item Each evening at 6pm when your store closes, you have the choice to order a certain number of bicycles from your supplier (including the option to not order any bicycles on a given day). 
	 \item The ordered bicycles will arrive 36 hours later (at 6am the day after the day after you orderâ€”we refer to this as delivery lead time of 36 hours). 
	 \item Denote the State at 6pm store-closing each day as $(\alpha, \beta)$, where $\alpha$ is the inventory in the store (referred to as On-Hand Inventory at 6pm) and $\beta$ is the inventory on a truck from the supplier (that you had ordered the previous day) that will arrive in your store the next morning at 6am ($\beta$ is referred to as On-Order Inventory at 6pm). Due to your storage capacity constraint of at most $C$ bicycles, your ordering policy is to order $C-(\alpha + \beta)$ if $\alpha + \beta < C$ and to not order if $\alpha + \beta \geq C$. The precise sequence of events in a 24-hour cycle is:
 \end{itemize}
 
In sum,
\begin{itemize}
	\item Observe the $(\alpha, \beta)$ \textit{State} at 6pm store-closing (call this state $S_t$).
	\item Immediately order according to the ordering policy described above.
	\item Receive bicycles at 6am if you had ordered 36 hours ago.
	\item Open the store at 8am.
	\item Experience random demand from customers according to demand probabilities stated above (number of bicycles sold for the day will be the minimum of demand on the day and inventory at store opening on the day).
	\item Close the store at 6pm and observe the state (this state is $S_{t+1}$).
\end{itemize}

If we let this process run for a while, in steady-state, we ensure that $\alpha + \beta \leq C$. So to model this process as a Finite Markov Process, we shall only consider the steady-state (finite) set of states
$$\mathcal{S} = \{(\alpha, \beta) | \alpha \in \mathbb{Z}_{\geq 0}, \beta \in \mathbb{Z}_{\geq 0}, 0 \leq \alpha + \beta \leq C\}$$
So restricting ourselves to this finite set of states, our order quantity equals $C - (\alpha + \beta)$ when the state is $(\alpha, \beta)$.

If the current state $S_t$ is $(\alpha, \beta)$, there are only $\alpha + \beta + 1$ possible next states $S_{t+1}$ as follows\footnotemark:
$$(\alpha + \beta - i, C - (\alpha + \beta)) \text{ for } i =0, 1, \ldots, \alpha + \beta$$
with transition probabilities governed by the Poisson probabilities of demand as follows:
\begin{align*}
	&\mathcal{P}((\alpha, \beta), (\alpha + \beta - i, C - (\alpha + \beta))) = f(i)\text{ for } 0 \leq i \leq \alpha + \beta - 1\\
	&\mathcal{P}((\alpha, \beta), (0, C - (\alpha + \beta))) = \sum_{j=\alpha+\beta}^{\infty} f(j) = 1 - F(\alpha + \beta - 1)
\end{align*}
\begin{itemize}
	\item $P((\alpha, \beta), (\alpha + \beta - i, C - (\alpha + \beta)))$ is the probability of transitioning from state $(\alpha, \beta)$ to $(\alpha + \beta - i, C - (\alpha + \beta))$ when demand equals $i$. The probability $f(i)$ is based on the Poisson demand distribution.
	\item If demand exceeds $\alpha + \beta$, the system transitions to a state where all on-hand inventory is depleted:
		\begin{align*}
			\mathbb{P}((\alpha, \beta), (0, C - (\alpha + \beta))) = 1 - F(\alpha + \beta - 1)
		\end{align*}
	\item $1 - F(\alpha + \beta - 1)$ gives the probability that the demand is greater than or equal to $\alpha+\beta$ (\ie demand is large enough to deplete the entire on-hand inventory).
\end{itemize}
\footnotetext{Since $\alpha+\beta=C$, there are only $C+1$ states. Also, the number of bicycles is the sum of the number of bicycles in the inventory and the truck. Thus, the next state's bicycles is $\alpha+\beta+i$}


\section{Bellman Equation}
\label{appendix:bellman_equation}

\textit{Bellman equation} can be derived as follows:

\begin{align*}
	v_\pi(s) &= \mathbb{E}_{\pi}[G_t|S_t = s]\\ 
	&= \mathbb{E}_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t = s]\\ 
	&= \mathbb{E}_{\pi}[R_{t+1}|S_t = s] + \mathbb{E}_{\pi}[\gamma G_{t+1}|S_t = s], \quad \text{By Linearity of Expectation.}\\
	&= \sum_{r_{t+1}}r_{t+1} P(R_{t+1}|S_t = s) + \mathbb{E}_{\pi}[\gamma G_{t+1}|S_t = s]\\
	&= \sum_{r_{t+1}}r_{t+1} \sum_a P(R_{t+1}|S_t = s, A_t=a)P(A_t=a|S_t=s) + \mathbb{E}_{\pi}[\gamma G_{t+1}|S_t = s]\\
	&=\sum_a \sum_{r_{t+1}}r_{t+1} \sum_{s'}  P(R_{t+1}, S_{t+1}=s'|S_t = s, A_t=a)P(A_t=a|S_t=s) + \mathbb{E}_{\pi}[\gamma G_{t+1}|S_t = s]\\
&=\sum_a \sum_{r}r \sum_{s'} P(s',r|s, a)\pi(a|s) + \mathbb{E}_{\pi}[\gamma G_{t+1}|S_t = s]\\
&=\sum_a \sum_{s'} \sum_{r}r P(s',r|s, a)\pi(a|s) + \mathbb{E}_{\pi}[\gamma G_{t+1}|S_t = s]\\
&=\sum_a \sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a \pi(a|s)+ \gamma \mathbb{E}_{\pi}[ G_{t+1}|S_t = s]\\
&=\sum_a \sum_{s'}\mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a \pi(a|s) + \gamma \sum_{a}\mathbb{E}_{\pi}[ G_{t+1}|S_t = s, A_{t}=a]P(A_{t}|S_{t})\\
&=\sum_a \pi(a|s) \Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \mathbb{E}_{\pi}[ G_{t+1}|S_t = s, A_{t}=a]\Bigg]\\
&=\sum_a \pi(a|s)\Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \sum_{g_{t+1}}g_{t+1}P( G_{t+1}|S_t = s, A_{t}=a)\Bigg]\\
&=\sum_a \pi(a|s) \Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \sum_{g_{t+1}}g_{t+1}\frac{P( G_{t+1},S_t = s, A_{t}=a)}{P(S_t = s, A_{t}=a)}\Bigg]\\
&=\sum_a \pi(a|s)\Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \sum_{g_{t+1}}g_{t+1}\frac{\sum_{s'}P( G_{t+1},S_t = s, S_{t+1}=s', A_{t}=a)}{P(S_t = s, A_{t}=a)}\Bigg]\\
&=\sum_a \pi(a|s)\Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \sum_{g_{t+1}}g_{t+1}\frac{\sum_{s'}P( G_{t+1}|s, s', a)P(s, s', a)}{P(s, a)}\Bigg]\\
&=\sum_a \pi(a|s)\Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \sum_{g_{t+1}}g_{t+1}\sum_{s'}P( G_{t+1}|s, s', a)P(s'| s, a)\Bigg]\\
&=\sum_a \pi(a|s)\Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \sum_{s'}P(s'| s, a)\sum_{g_{t+1}}g_{t+1}P( G_{t+1}|s')\Bigg] \quad \textrm{by Markov Property}\\
&=\sum_a \pi(a|s)\Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \sum_{s'}\mathcal{P}_{ss'}^a v_\pi(s')\Bigg] \\
&=\sum_a \pi(a|s)\sum_{s'}\mathcal{P}_{ss'}^a\Bigg[\mathcal{R}_{ss'}^a + \gamma  v_\pi(s')\Bigg] \\
&=\sum_a \pi(a|s)\sum_{r,s'}p(s',r|s,a)\Big[r + \gamma  v_\pi(s')\Big] 
\end{align*}

Or simply, 
\begin{align*}
	v_\pi(s) &= \sum_a \pi(a|s)q(s,a)\\
	&=\sum_a \pi(a|s)\sum_{r,s'}p(s',r|s,a)\Big[r + \gamma  v_\pi(s')\Big] 
\end{align*}
%The expectation here describes what we expect the return to be if we continue from state s following policy $\pi$. The expectation can be written explicitly by summing over all possible actions and all possible returned states. The next two equations can help us make the next step.

\href{https://stats.stackexchange.com/questions/243384/deriving-bellmans-equation-in-reinforcement-learning}{Reference}

Similarly,
\begin{eqnarray*}
q_\pi(s,a) &=& \mathbb{E}_\pi[G_t|S_t=s,A_t=a]\\
&=&\mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1}|S_t=s,A_t=a]\\
&=&\mathbb{E}_\pi[R_{t+1}|S_t=s,A_t=a] + \gamma\mathbb{E}_\pi[G_{t+1}|S_t=s,A_t=a]\\
&=&\sum_r rp(r|s,a) + \gamma\mathbb{E}_\pi[G_{t+1}|S_t=s,A_t=a]\\
&=&\sum_r r\sum_{s'}p(s',r|s,a) + \gamma\mathbb{E}_\pi[G_{t+1}|S_t=s,A_t=a]\\
&=&\sum_{s',r}rp(s',r|s,a) + \gamma\mathbb{E}[\mathbb{E}_\pi[G_{t+1}|S_t=s,A_t=a,R_{t+1},S_{t+1}]] \quad \text{By Law of Total Expectation.}\\
&=&\sum_{s',r} rp(s',r|s,a) + \gamma\sum_{s',r}\mathbb{E}_\pi[G_{t+1}|S_t=s,A_t=a,R_{t+1}=r,S_{t+1}=s']p(s',r|s,a)\\
&=&\sum_{s',r} p(s',r|s,a)[r + \gamma\mathbb{E}_\pi[G_{t+1}|S_t=s,A_t=a,R_{t+1}=r,S_{t+1}=s']\\
	&=&\sum_{s',r} p(s',r|s,a)[r + \gamma\mathbb{E}_\pi[G_{t+1}|S_{t+1}=s'] \quad \text{By Markov Property.}\\
&=&\sum_{s',r} p(s',r|s,a)[r + \gamma v_\pi(s')]\\
\end{eqnarray*}

\section{Importance Sampling}
You have two distributions, $P(A)$ and $P(B)$, and you have a sequence sampled from $A$. You can estimate an expectation of $A$, 

$$\mathbb{E}[A] = \sum p(a)h(a).$$

Can we use the above equation for estimating an expectation of B? Yes.

$$\mathbb{E}[B] = \sum \frac{p(b)}{p(a)}h(a).$$
The ratio $\frac{p(b)}{p(a)}$ tells us how likely to observe some results under $p(b)$ compared to $p(a)$.



%\begin{align*}
%	q_\pi(s, a) &= \mathbb{E}_{\pi}[G_t|S_t = s, A_t=a]\\ 
%	&= \mathbb{E}_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t = s, A_t=a]\\ 
%	&= \sum_{s'}\mathbb{E}_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t = s, A_t=a, S_{t+1}=s'] P(S_{t+1}=s'|S_t=s, A_t=a)\\ 
%	&= \sum_{s'}\mathbb{E}_{\pi}[R_{t+1}|S_t = s, A_t=a, S_{t+1}=s'] \mathcal{P}_{ss'}^a + \gamma\sum_{s'}\mathbb{E}_{\pi}[G_{t+1}|S_t = s, A_t=a, S_{t+1}=s'] \mathcal{P}_{ss'}^a \\ 
%	&= \sum_{s'}\mathbb{E}_{\pi}[R_{t+1}|S_t = s, A_t=a, S_{t+1}=s'] \mathcal{P}_{ss'}^a + \gamma\sum_{s'}\mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s'] \mathcal{P}_{ss'}^a \\ 
%	&= \sum_{s'} \sum_r r p(s',r|s,a) + \gamma\sum_{s'}v_\pi(s') \mathcal{P}_{ss'}^a \\ 
%	&= \sum_{s'}\sum_r r p(s',r|s,a) + \gamma\sum_{s'}\sum_{a'}q_\pi(s',a')\pi(a'|s') \mathcal{P}_{ss'}^a
%\end{align*}
%
%\section{Laplace Approximation}
%It works well
%
%\section{Regularized Logistic Regression}
%
%\begin{align*}
%	-\log p(\boldsymbol{w}|\boldsymbol{x}) &= -\underbrace{\log p(\boldsymbol{x}|\boldsymbol{w})}_{likelihood} - \underbrace{\log p(\boldsymbol{w})}_{Prior} + \text{const.} \\
%	&= \sum\limits_j \log \left( 1 + \exp(-y_j \boldsymbol{w}^\top \boldsymbol{x}_j) \right) + \sum\limits_i \frac{q_i (w_i - m_i)^2}{2} + \text{const.}'
%\end{align*}
Useful factorization of conditional probability:
\begin{align}
P[A,B|C]&=\frac{P[A,B,C]}{P[C]} \\
&= \frac{P[A,B,C]}{P[C]} \frac{P[B,C]}{P[B,C]}\\
&= \frac{P[A,B,C]}{P[B,C]} \frac{P[B,C]}{P[C]}\\
&= P[A|B,C] P[B|C]
\end{align}


\section{Fisher Information}
Suppose we have a model parameterized by parameter vector $\theta$ that models a distribution $p(x;\theta)$. In frequentist statistics, the way we learn $\theta$ is to maximize the likelihood of $p(x;\theta)$. To assess the goodness of our estimate of $\theta$ we define a \textbf{score function} as follows:
\begin{align*}
	f(\theta) = \nabla_\theta \log p(x;\theta).
\end{align*}
The expected value of score function is zero. 
\begin{align*}
	\mathbb{E}_{p(x;\theta)}[f(\theta)] &= \mathbb{E}_{p(x;\theta)}[\nabla_\theta \log p(x;\theta)]\\
	&= \int p(x;\theta) \nabla_\theta \log p(x;\theta) dx\\
	&= \int p(x;\theta) \frac{\nabla_\theta p(x;\theta)}{p(x;\theta)}  dx\\
	&= 0
\end{align*}
The covariance of the score function is given by
\begin{align*}
	\textrm{Cov}[f(\theta), f(\theta)] &= \mathbb{E}_{p(x;\theta)}[(f(\theta)-0)(f(\theta)-0)^T]= \textrm{Var}[f(\theta), f(\theta)].
\end{align*}
This the definition of Fisher information and it can be written 
\begin{align*}
	F = \mathbb{E}_{p(x;\theta)}[\nabla \log p(x;\theta)\nabla \log p(x;\theta)^T].
\end{align*}
Empirically, 
\begin{align*}
	F = \frac{1}{N}\sum_{i=1}^{N}\nabla \log p(x;\theta)\nabla \log p(x;\theta)^T.
\end{align*}

\section{Score Function}
In statistics, \textit{the score (or informant) is the gradient of the log-likelihood function with respect to the parameter vector}. 
\begin{itemize}
	\item Evaluated at a particular point of the parameter vector, the score indicates the \textbf{steepness of the log-likelihood function and thereby the sensitivity to infinitesimal changes to the parameter values}.
	\item If the log-likelihood function is continuous over the parameter space, the score will \textbf{vanish at a local maximum or minimum}; this fact is used in maximum likelihood estimation to find the parameter values that maximize the likelihood function.
\end{itemize}

Since the score is a function of the observations that are subject to sampling error, it lends itself to a test statistic known as score test in which the parameter is held at a particular value. Further, the ratio of two likelihood functions evaluated at two distinct parameter values can be understood as a definite integral of the score function.[2]

\section{Incremental Monte-Carlo}
Incremental Mean:
\begin{align}
	\mu_k &= \frac{1}{k}\sum_{j=1}^k x_j\\
	&= \frac{1}{k}\Big(x_k+(k-1)\frac{1}{(k-1)}\sum_{j=1}^{k-1} x_j\Big)\\
	&= \frac{1}{k}\Big(x_k+(k-1)\mu_{k-1}\Big)\\
	&= \mu_{k-1}+\frac{1}{k}(x_k-\mu_{k-1})
	\label{eq:incremental_mean}
\end{align}

Incremental MC:
\begin{itemize}
	\item $N_{n+1}(S_t^n) = N_{n}(S_t^n) + 1$ for rest $N_{n+1}(s) = N_{n}(s)$
	\item $V_{n+1}(S_t) = V_{n}(S_t)+\frac{G_{t:T}^n -V_n(S_t)}{N_n(S_t)}$ for rest $V_{n+1}(s)= V_n(s)$ 
	\item $V_{n+1}(S_t) = V_{n}(S_t)+\alpha({G_{t:T}^n -V_n(S_t)})$ for rest $V_{n+1}(s)= V_n(s)$ 
\end{itemize}

\section{Derivative of Softmax}

Softmax function is given by
\begin{align*}
	S(x_{i}) = \frac{e^{x_i}}{\sum_{k=1}^K e^{x_k}} \;\;\;\text{ for } i = 1, \dots, K
\end{align*}

The derivative of softmax function is
\begin{align*}
\frac{\partial S_i}{\partial x_j} = 
\begin{cases}S_i(1-S_j) &\text{ if } i=j \\
-S_jS_i &\text{ if } i \neq j
\end{cases}
\end{align*}

\begin{itemize} 
	\item Diagoal elements: $S_i(1-S_j)$
	\item Off-diagonal elements: $-S_jS_i$:
\end{itemize}


The Jacobian matrix ($j\times i$) for softmax is 
\begin{align*}
	\frac{\partial S}{\partial x} =
\begin{bmatrix}
\frac{\partial S_{1}}{\partial x_{1}} & \frac{\partial S_{1}}{\partial x_{2}} & \cdots & \frac{\partial S_{1}}{\partial x_{K}} \\
\frac{\partial S_{2}}{\partial x_{1}} & \frac{\partial S_{2}}{\partial x_{2}} & \cdots & \frac{\partial S_{K}}{\partial x_{K}} \\
\vdots & \vdots & \cdots & \vdots \\
\frac{\partial S_{K}}{\partial x_{1}} & \frac{\partial S_{K}}{\partial x_{2}} & \cdots & \frac{\partial S_{K}}{\partial x_{K}} \\
	\end{bmatrix}
\end{align*}

The matrix can be expressed as follows:

\begin{align*}
\begin{bmatrix}
	S(x_1)-S(x_1)S(x_1) & \ldots & 0-S(x_1)S(x_N)\\
	\ldots & S(x_j)-S(x_j)S(x_i) & \ldots\\
	0-S(x_N)S(x_1)& \ldots & 0-S(x_N)S(x_N)
\end{bmatrix} =  \\
\begin{bmatrix}
	S(x_1) & \ldots & 0\\
	\ldots & S(x_j) & \ldots\\
	0& \ldots & S(x_N)
\end{bmatrix} - 
\begin{bmatrix}
	S(x_1)S(x_1) & \ldots & S(x_1)S(x_N)\\
	\ldots & S(x_j)S(x_i) & \ldots\\
	S(x_N)S(x_1)& \ldots & S(x_N)S(x_N)
\end{bmatrix}
\end{align*}
This can be 

\begin{lstlisting}[language=Python]
np.diag(S) - np.outer(S, S)
\end{lstlisting}

\input{./appendix/policy_gradient_theorem}

\end{appendices}

