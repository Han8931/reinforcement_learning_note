\renewcommand{\thesection}{\Alph{section}.\arabic{section}}
\setcounter{section}{0}

\begin{appendices}
\chapter{Appendix}

\section{Bellman Equation}
\label{appendix:bellman_equation}

\textit{Bellman equation} can be derived as follows:

\begin{align*}
	v_\pi(s) &= \mathbb{E}_{\pi}[G_t|S_t = s]\\ 
	&= \mathbb{E}_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t = s]\\ 
	&= \mathbb{E}_{\pi}[R_{t+1}|S_t = s] + \mathbb{E}_{\pi}[\gamma G_{t+1}|S_t = s], \quad \text{By Linearity of Expectation.}\\
	&= \sum_{r_{t+1}}r_{t+1} P(R_{t+1}|S_t = s) + \mathbb{E}_{\pi}[\gamma G_{t+1}|S_t = s]\\
	&= \sum_{r_{t+1}}r_{t+1} \sum_a P(R_{t+1}|S_t = s, A_t=a)P(A_t=a|S_t=s) + \mathbb{E}_{\pi}[\gamma G_{t+1}|S_t = s]\\
	&=\sum_a \sum_{r_{t+1}}r_{t+1} \sum_{s'}  P(R_{t+1}, S_{t+1}=s'|S_t = s, A_t=a)P(A_t=a|S_t=s) + \mathbb{E}_{\pi}[\gamma G_{t+1}|S_t = s]\\
&=\sum_a \sum_{r}r \sum_{s'} P(s',r|s, a)\pi(a|s) + \mathbb{E}_{\pi}[\gamma G_{t+1}|S_t = s]\\
&=\sum_a \sum_{s'} \sum_{r}r P(s',r|s, a)\pi(a|s) + \mathbb{E}_{\pi}[\gamma G_{t+1}|S_t = s]\\
&=\sum_a \sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a \pi(a|s)+ \gamma \mathbb{E}_{\pi}[ G_{t+1}|S_t = s]\\
&=\sum_a \sum_{s'}\mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a \pi(a|s) + \gamma \sum_{a}\mathbb{E}_{\pi}[ G_{t+1}|S_t = s, A_{t}=a]P(A_{t}|S_{t})\\
&=\sum_a \pi(a|s) \Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \mathbb{E}_{\pi}[ G_{t+1}|S_t = s, A_{t}=a]\Bigg]\\
&=\sum_a \pi(a|s)\Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \sum_{g_{t+1}}g_{t+1}P( G_{t+1}|S_t = s, A_{t}=a)\Bigg]\\
&=\sum_a \pi(a|s) \Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \sum_{g_{t+1}}g_{t+1}\frac{P( G_{t+1},S_t = s, A_{t}=a)}{P(S_t = s, A_{t}=a)}\Bigg]\\
&=\sum_a \pi(a|s)\Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \sum_{g_{t+1}}g_{t+1}\frac{\sum_{s'}P( G_{t+1},S_t = s, S_{t+1}=s', A_{t}=a)}{P(S_t = s, A_{t}=a)}\Bigg]\\
&=\sum_a \pi(a|s)\Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \sum_{g_{t+1}}g_{t+1}\frac{\sum_{s'}P( G_{t+1}|s, s', a)P(s, s', a)}{P(s, a)}\Bigg]\\
&=\sum_a \pi(a|s)\Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \sum_{g_{t+1}}g_{t+1}\sum_{s'}P( G_{t+1}|s, s', a)P(s'| s, a)\Bigg]\\
&=\sum_a \pi(a|s)\Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \sum_{s'}P(s'| s, a)\sum_{g_{t+1}}g_{t+1}P( G_{t+1}|s')\Bigg] \quad \textrm{by Markov Property}\\
&=\sum_a \pi(a|s)\Bigg[\sum_{s'} \mathcal{P}_{ss'}^a\mathcal{R}_{ss'}^a + \gamma \sum_{s'}\mathcal{P}_{ss'}^a v_\pi(s')\Bigg] \\
&=\sum_a \pi(a|s)\sum_{s'}\mathcal{P}_{ss'}^a\Bigg[\mathcal{R}_{ss'}^a + \gamma  v_\pi(s')\Bigg] \\
&=\sum_a \pi(a|s)\sum_{r,s'}p(s',r|s,a)\Big[r + \gamma  v_\pi(s')\Big] 
\end{align*}

Or simply, 
\begin{align*}
	v_\pi(s) &= \sum_a \pi(a|s)q(s,a)\\
	&=\sum_a \pi(a|s)\sum_{r,s'}p(s',r|s,a)\Big[r + \gamma  v_\pi(s')\Big] 
\end{align*}
%The expectation here describes what we expect the return to be if we continue from state s following policy $\pi$. The expectation can be written explicitly by summing over all possible actions and all possible returned states. The next two equations can help us make the next step.

\href{https://stats.stackexchange.com/questions/243384/deriving-bellmans-equation-in-reinforcement-learning}{Reference}

Similarly,
\begin{eqnarray*}
q_\pi(s,a) &=& \mathbb{E}_\pi[G_t|S_t=s,A_t=a]\\
&=&\mathbb{E}_\pi[R_{t+1} + \gamma G_{t+1}|S_t=s,A_t=a]\\
&=&\mathbb{E}_\pi[R_{t+1}|S_t=s,A_t=a] + \gamma\mathbb{E}_\pi[G_{t+1}|S_t=s,A_t=a]\\
&=&\sum_r rp(r|s,a) + \gamma\mathbb{E}_\pi[G_{t+1}|S_t=s,A_t=a]\\
&=&\sum_r r\sum_{s'}p(s',r|s,a) + \gamma\mathbb{E}_\pi[G_{t+1}|S_t=s,A_t=a]\\
&=&\sum_{s',r}rp(s',r|s,a) + \gamma\mathbb{E}[\mathbb{E}_\pi[G_{t+1}|S_t=s,A_t=a,R_{t+1},S_{t+1}]] \quad \text{By Law of Total Expectation.}\\
&=&\sum_{s',r} rp(s',r|s,a) + \gamma\sum_{s',r}\mathbb{E}_\pi[G_{t+1}|S_t=s,A_t=a,R_{t+1}=r,S_{t+1}=s']p(s',r|s,a)\\
&=&\sum_{s',r} p(s',r|s,a)[r + \gamma\mathbb{E}_\pi[G_{t+1}|S_t=s,A_t=a,R_{t+1}=r,S_{t+1}=s']\\
	&=&\sum_{s',r} p(s',r|s,a)[r + \gamma\mathbb{E}_\pi[G_{t+1}|S_{t+1}=s'] \quad \text{By Markov Property.}\\
&=&\sum_{s',r} p(s',r|s,a)[r + \gamma v_\pi(s')]\\
\end{eqnarray*}

\section{Importance Sampling}
You have two distributions, $P(A)$ and $P(B)$, and you have a sequence sampled from $A$. You can estimate an expectation of $A$, 

$$\mathbb{E}[A] = \sum p(a)h(a).$$

Can we use the above equation for estimating an expectation of B? Yes.

$$\mathbb{E}[B] = \sum \frac{p(b)}{p(a)}h(a).$$
The ratio $\frac{p(b)}{p(a)}$ tells us how likely to observe some results under $p(b)$ compared to $p(a)$.



%\begin{align*}
%	q_\pi(s, a) &= \mathbb{E}_{\pi}[G_t|S_t = s, A_t=a]\\ 
%	&= \mathbb{E}_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t = s, A_t=a]\\ 
%	&= \sum_{s'}\mathbb{E}_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t = s, A_t=a, S_{t+1}=s'] P(S_{t+1}=s'|S_t=s, A_t=a)\\ 
%	&= \sum_{s'}\mathbb{E}_{\pi}[R_{t+1}|S_t = s, A_t=a, S_{t+1}=s'] \mathcal{P}_{ss'}^a + \gamma\sum_{s'}\mathbb{E}_{\pi}[G_{t+1}|S_t = s, A_t=a, S_{t+1}=s'] \mathcal{P}_{ss'}^a \\ 
%	&= \sum_{s'}\mathbb{E}_{\pi}[R_{t+1}|S_t = s, A_t=a, S_{t+1}=s'] \mathcal{P}_{ss'}^a + \gamma\sum_{s'}\mathbb{E}_{\pi}[G_{t+1}|S_{t+1}=s'] \mathcal{P}_{ss'}^a \\ 
%	&= \sum_{s'} \sum_r r p(s',r|s,a) + \gamma\sum_{s'}v_\pi(s') \mathcal{P}_{ss'}^a \\ 
%	&= \sum_{s'}\sum_r r p(s',r|s,a) + \gamma\sum_{s'}\sum_{a'}q_\pi(s',a')\pi(a'|s') \mathcal{P}_{ss'}^a
%\end{align*}
%
%\section{Laplace Approximation}
%It works well
%
%\section{Regularized Logistic Regression}
%
%\begin{align*}
%	-\log p(\boldsymbol{w}|\boldsymbol{x}) &= -\underbrace{\log p(\boldsymbol{x}|\boldsymbol{w})}_{likelihood} - \underbrace{\log p(\boldsymbol{w})}_{Prior} + \text{const.} \\
%	&= \sum\limits_j \log \left( 1 + \exp(-y_j \boldsymbol{w}^\top \boldsymbol{x}_j) \right) + \sum\limits_i \frac{q_i (w_i - m_i)^2}{2} + \text{const.}'
%\end{align*}
Useful factorization of conditional probability:
\begin{align}
P[A,B|C]&=\frac{P[A,B,C]}{P[C]} \\
&= \frac{P[A,B,C]}{P[C]} \frac{P[B,C]}{P[B,C]}\\
&= \frac{P[A,B,C]}{P[B,C]} \frac{P[B,C]}{P[C]}\\
&= P[A|B,C] P[B|C]
\end{align}


\section{Fisher Information}
Suppose we have a model parameterized by parameter vector $\theta$ that models a distribution $p(x;\theta)$. In frequentist statistics, the way we learn $\theta$ is to maximize the likelihood of $p(x;\theta)$. To assess the goodness of our estimate of $\theta$ we define a \textbf{score function} as follows:
\begin{align*}
	f(\theta) = \nabla_\theta \log p(x;\theta).
\end{align*}
The expected value of score function is zero. 
\begin{align*}
	\mathbb{E}_{p(x;\theta)}[f(\theta)] &= \mathbb{E}_{p(x;\theta)}[\nabla_\theta \log p(x;\theta)]\\
	&= \int p(x;\theta) \nabla_\theta \log p(x;\theta) dx\\
	&= \int p(x;\theta) \frac{\nabla_\theta p(x;\theta)}{p(x;\theta)}  dx\\
	&= 0
\end{align*}
The covariance of the score function is given by
\begin{align*}
	\textrm{Cov}[f(\theta), f(\theta)] &= \mathbb{E}_{p(x;\theta)}[(f(\theta)-0)(f(\theta)-0)^T]= \textrm{Var}[f(\theta), f(\theta)].
\end{align*}
This the definition of Fisher information and it can be written 
\begin{align*}
	F = \mathbb{E}_{p(x;\theta)}[\nabla \log p(x;\theta)\nabla \log p(x;\theta)^T].
\end{align*}
Empirically, 
\begin{align*}
	F = \frac{1}{N}\sum_{i=1}^{N}\nabla \log p(x;\theta)\nabla \log p(x;\theta)^T.
\end{align*}

\section{Score Function}
In statistics, \textit{the score (or informant) is the gradient of the log-likelihood function with respect to the parameter vector}. 
\begin{itemize}
	\item Evaluated at a particular point of the parameter vector, the score indicates the \textbf{steepness of the log-likelihood function and thereby the sensitivity to infinitesimal changes to the parameter values}.
	\item If the log-likelihood function is continuous over the parameter space, the score will \textbf{vanish at a local maximum or minimum}; this fact is used in maximum likelihood estimation to find the parameter values that maximize the likelihood function.
\end{itemize}

Since the score is a function of the observations that are subject to sampling error, it lends itself to a test statistic known as score test in which the parameter is held at a particular value. Further, the ratio of two likelihood functions evaluated at two distinct parameter values can be understood as a definite integral of the score function.[2]

\section{Incremental Monte-Carlo}
Incremental Mean:
\begin{align}
	\mu_k &= \frac{1}{k}\sum_{j=1}^k x_j\\
	&= \frac{1}{k}\Big(x_k+(k-1)\frac{1}{(k-1)}\sum_{j=1}^{k-1} x_j\Big)\\
	&= \frac{1}{k}\Big(x_k+(k-1)\mu_{k-1}\Big)\\
	&= \mu_{k-1}+\frac{1}{k}(x_k-\mu_{k-1})
	\label{eq:incremental_mean}
\end{align}

Incremental MC:
\begin{itemize}
	\item $N_{n+1}(S_t^n) = N_{n}(S_t^n) + 1$ for rest $N_{n+1}(s) = N_{n}(s)$
	\item $V_{n+1}(S_t) = V_{n}(S_t)+\frac{G_{t:T}^n -V_n(S_t)}{N_n(S_t)}$ for rest $V_{n+1}(s)= V_n(s)$ 
	\item $V_{n+1}(S_t) = V_{n}(S_t)+\alpha({G_{t:T}^n -V_n(S_t)})$ for rest $V_{n+1}(s)= V_n(s)$ 
\end{itemize}

\section{Derivative of Softmax}

Softmax function is given by
\begin{align*}
	S(x_{i}) = \frac{e^{x_i}}{\sum_{k=1}^K e^{x_k}} \;\;\;\text{ for } i = 1, \dots, K
\end{align*}

The derivative of softmax function is
\begin{align*}
\frac{\partial S_i}{\partial x_j} = 
\begin{cases}S_i(1-S_j) &\text{ if } i=j \\
-S_jS_i &\text{ if } i \neq j
\end{cases}
\end{align*}

\begin{itemize} 
	\item Diagoal elements: $S_i(1-S_j)$
	\item Off-diagonal elements: $-S_jS_i$:
\end{itemize}


The Jacobian matrix ($j\times i$) for softmax is 
\begin{align*}
	\frac{\partial S}{\partial x} =
\begin{bmatrix}
\frac{\partial S_{1}}{\partial x_{1}} & \frac{\partial S_{1}}{\partial x_{2}} & \cdots & \frac{\partial S_{1}}{\partial x_{K}} \\
\frac{\partial S_{2}}{\partial x_{1}} & \frac{\partial S_{2}}{\partial x_{2}} & \cdots & \frac{\partial S_{K}}{\partial x_{K}} \\
\vdots & \vdots & \cdots & \vdots \\
\frac{\partial S_{K}}{\partial x_{1}} & \frac{\partial S_{K}}{\partial x_{2}} & \cdots & \frac{\partial S_{K}}{\partial x_{K}} \\
	\end{bmatrix}
\end{align*}

The matrix can be expressed as follows:

\begin{align*}
\begin{bmatrix}
	S(x_1)-S(x_1)S(x_1) & \ldots & 0-S(x_1)S(x_N)\\
	\ldots & S(x_j)-S(x_j)S(x_i) & \ldots\\
	0-S(x_N)S(x_1)& \ldots & 0-S(x_N)S(x_N)
\end{bmatrix} =  \\
\begin{bmatrix}
	S(x_1) & \ldots & 0\\
	\ldots & S(x_j) & \ldots\\
	0& \ldots & S(x_N)
\end{bmatrix} - 
\begin{bmatrix}
	S(x_1)S(x_1) & \ldots & S(x_1)S(x_N)\\
	\ldots & S(x_j)S(x_i) & \ldots\\
	S(x_N)S(x_1)& \ldots & S(x_N)S(x_N)
\end{bmatrix}
\end{align*}
This can be 

\begin{lstlisting}[language=Python]
np.diag(S) - np.outer(S, S)
\end{lstlisting}

\input{./appendix/policy_gradient_theorem}

\end{appendices}

