\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Introduction}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Sequential Decision Making}{1}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Exploration and Exploitation}{2}{subsection.1.1.2}%
\contentsline {section}{\numberline {1.2}Markov Chain}{2}{section.1.2}%
\contentsline {section}{\numberline {1.3}Multi-Armed Bandits}{4}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Action-value Methods}{4}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Incremental Implementation of The Action-value Methods}{5}{subsection.1.3.2}%
\contentsline {chapter}{\numberline {2}Markov Decision Process}{6}{chapter.2}%
\contentsline {section}{\numberline {2.1}Policies and Value Functions}{8}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}The State-Value Functions}{8}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}The Action-Value Function}{9}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}The Action-Advantage Function}{10}{subsection.2.1.3}%
\contentsline {section}{\numberline {2.2}Optimality}{10}{section.2.2}%
\contentsline {chapter}{\numberline {3}Dynamic Programming}{13}{chapter.3}%
\contentsline {section}{\numberline {3.1}Policy Evaluation}{13}{section.3.1}%
\contentsline {section}{\numberline {3.2}Policy Improvement}{14}{section.3.2}%
\contentsline {section}{\numberline {3.3}Value Iteration}{14}{section.3.3}%
\contentsline {chapter}{\numberline {4}Monte Carlo (Prediction) Methods}{15}{chapter.4}%
\contentsline {section}{\numberline {4.1}First Visit vs. Every Visit}{15}{section.4.1}%
\contentsline {section}{\numberline {4.2}On-Policy vs. Off-Policy}{15}{section.4.2}%
\contentsline {section}{\numberline {4.3}On Policy}{15}{section.4.3}%
\contentsline {section}{\numberline {4.4}Off Policy}{16}{section.4.4}%
\contentsline {section}{\numberline {4.5}Temporal-Difference Learninig}{17}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Sarsa: On-policy TD Control}{17}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}Q-learning: Off-policy TD Control}{17}{subsection.4.5.2}%
\contentsline {chapter}{\numberline {5}Deep Reinforcement Learning}{18}{chapter.5}%
\contentsline {chapter}{\numberline {6}Deep Q-Network}{19}{chapter.6}%
\contentsline {chapter}{\numberline {7}Policy Gradient}{20}{chapter.7}%
\contentsline {section}{\numberline {7.1}Natural Policy Gradient}{20}{section.7.1}%
\contentsline {subsection}{\numberline {7.1.1}KL-divergence between perturbed distributions}{20}{subsection.7.1.1}%
\contentsline {section}{\numberline {7.2}Proximal Policy Optimization}{22}{section.7.2}%
\contentsline {paragraph}{Positive Advantage:}{23}{section*.2}%
\contentsline {paragraph}{Negative Advantage:}{23}{section*.3}%
\contentsline {chapter}{Appendices}{25}{section*.5}%
\contentsline {chapter}{Appendix}{25}{appendix*.6}%
\contentsline {section}{\numberline {A.1}Bellman Equation}{25}{section.1..1}%
\contentsline {section}{\numberline {B.2}Importance Sampling}{27}{section.1..2}%
\contentsline {section}{\numberline {C.3}Fisher Information}{28}{section.1..3}%
\contentsline {section}{\numberline {D.4}Score Function}{28}{section.1..4}%
\contentsline {section}{\numberline {E.5}Incremental Monte-Carlo}{29}{section.1..5}%
\contentsline {section}{\numberline {F.6}Derivative of Softmax}{29}{section.1..6}%
\contentsline {section}{\numberline {G.7}Policy Gradient Theorem}{30}{section.1..7}%
\contentsline {subsection}{\numberline {G.7.1}Proof of Policy Gradient Theorem}{31}{subsection.1..7.1}%
