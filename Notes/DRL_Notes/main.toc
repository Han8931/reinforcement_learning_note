\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Markov Chain}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Multi-Armed Bandits}{3}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Action-value Methods}{3}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Incremental Implementation of The Action-value Methods}{4}{subsection.1.2.2}%
\contentsline {chapter}{\numberline {2}Markov Decision Process}{5}{chapter.2}%
\contentsline {section}{\numberline {2.1}Policies and Value Functions}{7}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}The State-Value Functions}{7}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}The Action-Value Function}{8}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}The Action-Advantage Function}{9}{subsection.2.1.3}%
\contentsline {section}{\numberline {2.2}Optimality}{9}{section.2.2}%
\contentsline {chapter}{\numberline {3}Dynamic Programming}{12}{chapter.3}%
\contentsline {section}{\numberline {3.1}Policy Evaluation}{12}{section.3.1}%
\contentsline {section}{\numberline {3.2}Policy Improvement}{12}{section.3.2}%
\contentsline {chapter}{\numberline {4}Monte Carlo Method}{13}{chapter.4}%
\contentsline {section}{\numberline {4.1}First Visit vs. Every Visit}{13}{section.4.1}%
\contentsline {section}{\numberline {4.2}On-Policy vs. Off-Policy}{13}{section.4.2}%
\contentsline {section}{\numberline {4.3}On Policy}{14}{section.4.3}%
\contentsline {section}{\numberline {4.4}Off Policy}{14}{section.4.4}%
\contentsline {section}{\numberline {4.5}Temporal-Difference Learninig}{15}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Sarsa: On-policy TD Control}{15}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}Q-learning: Off-policy TD Control}{15}{subsection.4.5.2}%
\contentsline {chapter}{\numberline {5}Policy Gradient}{16}{chapter.5}%
\contentsline {section}{\numberline {5.1}Natural Policy Gradient}{16}{section.5.1}%
\contentsline {subsection}{\numberline {5.1.1}KL-divergence between perturbed distributions}{16}{subsection.5.1.1}%
\contentsline {section}{\numberline {5.2}Proximal Policy Optimization}{18}{section.5.2}%
\contentsline {paragraph}{Positive Advantage:}{19}{section*.2}%
\contentsline {paragraph}{Negative Advantage:}{19}{section*.3}%
\contentsline {chapter}{Appendices}{21}{section*.5}%
\contentsline {chapter}{Appendix}{21}{appendix*.6}%
\contentsline {section}{\numberline {A.1}Bellman Equation}{21}{section.1..1}%
\contentsline {section}{\numberline {B.2}Importance Sampling}{23}{section.1..2}%
\contentsline {section}{\numberline {C.3}Fisher Information}{24}{section.1..3}%
\contentsline {section}{\numberline {D.4}Score Function}{24}{section.1..4}%
\contentsline {section}{\numberline {E.5}Incremental Monte-Carlo}{25}{section.1..5}%
