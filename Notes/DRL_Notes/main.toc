\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Introduction}{1}{section.1.1}%
\contentsline {subsection}{\numberline {1.1.1}Sequential Decision Making}{1}{subsection.1.1.1}%
\contentsline {subsection}{\numberline {1.1.2}Exploration and Exploitation}{2}{subsection.1.1.2}%
\contentsline {section}{\numberline {1.2}Markov Chain}{2}{section.1.2}%
\contentsline {section}{\numberline {1.3}Multi-Armed Bandits}{4}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Action-value Methods}{4}{subsection.1.3.1}%
\contentsline {subsection}{\numberline {1.3.2}Incremental Implementation of The Action-value Methods}{5}{subsection.1.3.2}%
\contentsline {chapter}{\numberline {2}Markov Decision Process}{6}{chapter.2}%
\contentsline {section}{\numberline {2.1}Policies and Value Functions}{8}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}The State-Value Functions}{8}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}The Action-Value Function}{9}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}The Action-Advantage Function}{10}{subsection.2.1.3}%
\contentsline {section}{\numberline {2.2}Optimality}{10}{section.2.2}%
\contentsline {chapter}{\numberline {3}Dynamic Programming}{13}{chapter.3}%
\contentsline {section}{\numberline {3.1}Policy Evaluation}{13}{section.3.1}%
\contentsline {section}{\numberline {3.2}Policy Improvement}{14}{section.3.2}%
\contentsline {section}{\numberline {3.3}Value Iteration}{14}{section.3.3}%
\contentsline {chapter}{\numberline {4}Monte Carlo Methods}{15}{chapter.4}%
\contentsline {section}{\numberline {4.1}Monte Carlo Prediction (Evaluation)}{15}{section.4.1}%
\contentsline {subsection}{\numberline {4.1.1}First Visit vs. Every Visit}{15}{subsection.4.1.1}%
\contentsline {section}{\numberline {4.2}Monte Carlo Control}{16}{section.4.2}%
\contentsline {section}{\numberline {4.3}On-Policy vs. Off-Policy}{16}{section.4.3}%
\contentsline {subsection}{\numberline {4.3.1}On Policy}{16}{subsection.4.3.1}%
\contentsline {subsection}{\numberline {4.3.2}Off Policy}{17}{subsection.4.3.2}%
\contentsline {section}{\numberline {4.4}Temporal-Difference Learninig}{18}{section.4.4}%
\contentsline {subsection}{\numberline {4.4.1}Sarsa: On-policy TD Control}{18}{subsection.4.4.1}%
\contentsline {subsection}{\numberline {4.4.2}Q-learning: Off-policy TD Control}{18}{subsection.4.4.2}%
\contentsline {chapter}{\numberline {5}Deep Reinforcement Learning}{19}{chapter.5}%
\contentsline {chapter}{\numberline {6}Deep Q-Network}{20}{chapter.6}%
\contentsline {chapter}{\numberline {7}Policy Gradient}{21}{chapter.7}%
\contentsline {subsection}{\numberline {7.0.1}Causality}{22}{subsection.7.0.1}%
\contentsline {section}{\numberline {7.1}Natural Policy Gradient}{22}{section.7.1}%
\contentsline {subsection}{\numberline {7.1.1}KL-divergence between perturbed distributions}{22}{subsection.7.1.1}%
\contentsline {section}{\numberline {7.2}Proximal Policy Optimization}{24}{section.7.2}%
\contentsline {paragraph}{Positive Advantage:}{25}{section*.2}%
\contentsline {paragraph}{Negative Advantage:}{25}{section*.3}%
\contentsline {chapter}{Appendices}{27}{section*.5}%
\contentsline {chapter}{Appendix}{27}{appendix*.6}%
\contentsline {section}{\numberline {A.1}Bellman Equation}{27}{section.1..1}%
\contentsline {section}{\numberline {B.2}Importance Sampling}{29}{section.1..2}%
\contentsline {section}{\numberline {C.3}Fisher Information}{30}{section.1..3}%
\contentsline {section}{\numberline {D.4}Score Function}{30}{section.1..4}%
\contentsline {section}{\numberline {E.5}Incremental Monte-Carlo}{31}{section.1..5}%
\contentsline {section}{\numberline {F.6}Derivative of Softmax}{31}{section.1..6}%
\contentsline {section}{\numberline {G.7}Policy Gradient Theorem}{32}{section.1..7}%
\contentsline {subsection}{\numberline {G.7.1}Proof of Policy Gradient Theorem}{33}{subsection.1..7.1}%
