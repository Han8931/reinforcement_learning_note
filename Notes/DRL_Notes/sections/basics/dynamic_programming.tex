\chapter{Dynamic Programming}
The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically. The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies.


Let $\pi$ and $\pi'$ be any pair of deterministic policies such that, for all $s\in \mathcal{S}$, 
\begin{align*}
	q_\pi(s, \pi'(s))\geq v_\pi(s).	
\end{align*}
Then, the policy $\pi'$ must be as good as, or better than $\pi$. Equivalently, it must obtain the following inequality for all states $s\in \mathcal{S}$:
\begin{align*}
	v_\pi'(s)\geq v_\pi(s).	
\end{align*}

\begin{theorem}[Policy Improvement Theorem]
	$q_\pi(s, \pi'(s))\geq v_\pi(s)$, then $v_\pi'(s)\geq v_\pi(s)$.	
	\label{thrm:policy_improvement}
\end{theorem}
Proof.

\begin{align*}
	v_\pi(s)&\leq q_\pi(s, \pi'(s))	\\
	&= \mathbb{E}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s, A_t=\pi'(s)]\\
	&= \mathbb{E}_{\pi'}[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]\\
	&\leq \mathbb{E}_{\pi'}[R_{t+1}+\gamma q_\pi(S_{t+1}, \pi'(S_{t+1}))|S_t=s]\\
	&= \mathbb{E}_{\pi'}[R_{t+1}+\gamma \mathbb{E}_{\pi'}[R_{t+2}+\gamma v_\pi(S_{t+2})|S_{t+1}, A_{t+1}=\pi'(S_{t+1})]\,|\,S_t=s]\\
	&= \mathbb{E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2 v_\pi(S_{t+2})|\,S_t=s]\\
	&\leq \mathbb{E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 v_\pi(S_{t+3})|\,S_t=s]\\
	&\vdots\\
	&\leq \mathbb{E}_{\pi'}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 R_{t+4}+\ldots|\,S_t=s]\\
	&= \mathbb{E}_{\pi'}[G_t|\,S_t=s]\\
	&= v_{\pi'}(s).
	%\label{eq:policy_improvement_theorem}
\end{align*}


\section{Policy Evaluation}
\begin{itemize}
	\item Prediction problem: refers to the problem of \textbf{evaluating policies} (sipmly, rating policies), of estimating value functions given a policy (learning to predict returns).
	\item Control problem: problem of \textbf{finding optimal policies}. Usually solved by the pattern of generalized policy iteration (GPI), where the competing processes of policy evaluation and policy improvement progressively move policies towards optimality.
	\item Policy evaluation: refers to algorithms that solve the prediction problem.
		\begin{itemize}
			\item Iterative policy evaluation:
				$$v_{k+1}(s)=\sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)[r + \gamma v_k(s')].$$ 
				\begin{enumerate}
					\item Init $v_0(s)$ for all $s$ arbitrarily and to 0 if $s$ is terminal. 
					\item Bootstrapping: $v_1(s)\to v_2(s)\to\cdots\to v_N(s)$
				\end{enumerate}
		\end{itemize}
\end{itemize}

\section{Policy Improvement}
Our reason for computing the value function for a policy is to help find better policies. The goal of policy improvement is to modify the policy to make it better with respect to the value function. This is done by selecting actions that are known to be better, according to the current value estimates.

\begin{itemize}
	\item The key is the action-value function, $q_\pi(s,a)$. We can selects the action that maximizes the action-value function in a greedy way: 
		\begin{align*}
			\pi'(s) &= \argmax_a q_\pi(s,a)\\
			&= \argmax_a \sum_{s',r}p(s',r|s,a)[r + \gamma v_\pi(s')].
		\end{align*}
	\item The greedy policy takes the action that looks best in the short term according to $v_\pi$.
\end{itemize}

The process of policy evaluation and improvement is often repeated iteratively until the policy converges to an optimal policy that maximizes the expected return in the given environment. This iterative process is the basis for algorithms like \textit{policy iteration} and \textit{value iteration} in reinforcement learning.


\section{Value Iteration}
One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set.

In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing the convergence guarantees of policy iteration.

\begin{align}
	v_{k+1}(s)=\max_a \sum_{s',r}p(s',r|s,a)[r + \gamma v_k(s')], \, \forall s\in \mathcal{S}.
	\label{eq:value_iteration}
\end{align}
For arbitrary $v_0$, the sequence $v_k$ can be shown to converge to $v_*$ under the same conditions that guarantee the existence of $v_*$.

