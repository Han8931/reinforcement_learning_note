\chapter{Dynamic Programming}
The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP). Classical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically. The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies.


Let $\pi$ and $\pi'$ be any pair of deterministic policies such that, for all $s\in \mathcal{S}$, 
\begin{align*}
	q_\pi(s, \pi'(s))\geq v_\pi(s).	
\end{align*}
Then, the policy $\pi'$ must be as good as, or better than $\pi$. Equivalently, it must obtain the following inequality for all states $s\in \mathcal{S}$:
\begin{align*}
	v_\pi'(s)\geq v_\pi(s).	
\end{align*}

\begin{theorem}[Policy Improvement Theorem]
	If $q_\pi(s, \pi'(s))\geq v_\pi(s)$, then 
	$$v_\pi'(s)\geq v_\pi(s)$$
	\label{thrm:policy_improvement}
\end{theorem}
%Proof.

%\begin{align*}
%	v_\pi(s)&\leq q_\pi(s, \pi'(s))	\\
%	&= \mathbb{E}[R_{t}+\gamma v_\pi(S_{t+1})|S_t=s, A_t=\pi'(s)]\\
%	&= \mathbb{E}_{\pi'}[R_{t}+\gamma v_\pi(S_{t+1})|S_t=s]\\
%	&\leq \mathbb{E}_{\pi'}[R_{t}+\gamma q_\pi(S_{t+1}, \pi'(S_{t+1}))|S_t=s]\\
%	&= \mathbb{E}_{\pi'}[R_{t}+\gamma \mathbb{E}_{\pi'}[R_{t+2}+\gamma v_\pi(S_{t+2})|S_{t+1}, A_{t+1}=\pi'(S_{t+1})]\,|\,S_t=s]\\
%	&= \mathbb{E}_{\pi'}[R_{t}+\gamma R_{t+2}+\gamma^2 v_\pi(S_{t+2})|\,S_t=s]\\
%	&\leq \mathbb{E}_{\pi'}[R_{t}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 v_\pi(S_{t+3})|\,S_t=s]\\
%	&\vdots\\
%	&\leq \mathbb{E}_{\pi'}[R_{t}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 R_{t+4}+\ldots|\,S_t=s]\\
%	&= \mathbb{E}_{\pi'}[G_t|\,S_t=s]\\
%	&= v_{\pi'}(s).
%	%\label{eq:policy_improvement_theorem}
%\end{align*}


\section{Policy Evaluation}
\begin{itemize}
	\item Prediction problem: refers to the problem of \textbf{evaluating policies} (sipmly, rating policies), of estimating value functions given a policy (learning to predict returns).
	\item Control problem: problem of \textbf{finding optimal policies}. Usually solved by the pattern of generalized policy iteration (GPI), where the competing processes of policy evaluation and policy improvement progressively move policies towards optimality.
	\item Policy evaluation: refers to algorithms that solve the prediction problem.
		\begin{itemize}
			\item Iterative policy evaluation:
				$$v_{k+1}(s)=\sum_{a}\pi(a|s)\sum_{s'}p(s'|s,a)[r + \gamma v_k(s')].$$ 
				\begin{enumerate}
					\item Init $v_0(s)$ for all $s$ arbitrarily and to 0 if $s$ is terminal. 
					\item Bootstrapping: $v_1(s)\to v_2(s)\to\cdots\to v_N(s)$
				\end{enumerate}
		\end{itemize}
\end{itemize}

A single round of updates, $k$ , involves updating all the state values. The algorithm stops until 
the changes in state values are sufficiently small in successive iterations.

% \section{Policy Iteration}

\section{Policy Improvement}
Our reason for computing the value function for a policy is to help find better policies. The goal of policy improvement is to modify the policy to make it better with respect to the value function. This is done by selecting actions that are known to be better, according to the current value estimates.

\begin{itemize}
	\item The key is the action-value function, $q_\pi(s,a)$. To improve a policy, we use a state-value function and an MDP to get a one-step look-ahead and determine which of the actions lead to the highest value. We can selects the action that \textit{maximizes the action-value function in a greedy way}: 
		\begin{align*}
			\pi'(s) &= \argmax_a q_\pi(s,a)\\
			&= \argmax_a \sum_{s'}p(s'|s,a)[r + \gamma v_\pi(s')].
		\end{align*}
	\item The greedy policy takes the action that looks best in the short term according to $v_\pi$.
	\item If the policy is not going to be updated, then the policy is the optimal policy.
\end{itemize}

The process of the policy evaluation and the policy improvement are often repeated iteratively until the policy converges to an optimal policy that maximizes the expected return in the given environment. This iterative process is the basis for algorithms like \textit{policy iteration} and \textit{value iteration} in reinforcement learning.

\section{Value Iteration}
One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set.

In fact, the policy evaluation step of policy iteration can be truncated in several ways without losing the convergence guarantees of policy iteration.

\begin{align}
	v_{k+1}(s)=\max_a \sum_{s'}p(s'|s,a)[r + \gamma v_k(s')], \, \forall s\in \mathcal{S}.
	\label{eq:value_iteration}
\end{align}
For arbitrary $v_0$, the sequence $v_k$ can be shown to converge to $v_*$ under the same conditions that guarantee the existence of $v_*$.

Key points:
\begin{itemize}
	\item Policy iteration includes: policy evaluation with improvement, and the two are repeated iteratively until policy converges.
	\item Value iteration includes: finding optimal value function and one policy extraction. There is no repeat of the two because once the value function is optimal, then the policy out of it should also be optimal (\ie. converged).
	\item Finding optimal value function can also be seen as a combination of policy improvement (due to max) and truncated policy evaluation (the reassignment of v_(s) after just one sweep of all states regardless of convergence).
	\item The algorithms for policy evaluation and finding optimal value function are highly similar except for a max operation (as highlighted)
	\item Similarly, the key step to policy improvement and policy extraction are identical except the former involves a stability check.
\end{itemize}
