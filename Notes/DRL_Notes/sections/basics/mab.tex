\section{Multi-Armed Bandits}

Bandit problems are stateless. Each arm has a fixed distribution of rewards. It does not depend on which arms were pulled previously. The goal is to explore the reward distributions of all arms and then keep pulling the best one. We only have a single chance of selecting an action in each episode. 

% Markov decision processes are a temporal extension of bandit problems: pulling an arm influences the future rewards. Technically, there is a state that changes by pulling an arm. The reward distributions depend on that state.

You can view bandit problems as Markov decision processes where all states are terminal. In that case, all decision sequences have a length of 1 and subsequent pulls don't influence each other.

In sum,
\begin{itemize}
	\item When the lever of a slot machine is pulled it gives a random reward coming from a probability distribution specific to that machine.
	\item Although the machines look identical, their reward probability distributions are different. 
	\item In each turn, gamblers need to decide whether to play the machine that has given the highest average reward so far, or to try another machine. 
\end{itemize}

In our $k$-armed bandit problem, each of the $k$ actions has an expected or mean reward given that that action is selected: let us call this the \textit{value} of that action. We denote the action selected on time step $t$ as $A_t$, and the corresponding reward as $R_t$. The value then of an arbitrary action $a$, denoted $q_*(a)$, is the expected reward given that $a$ is selected:
$$q_*(a) = \mathbb{E}[R_t|A_t=a].$$
Since we do not know which action is the best, we have to estimate the value of actions $a$ at time step $t$, $Q_t(a)$.

\begin{lstlisting}[language=Python]
class GaussianBandit:
    def __init__(self, mean=0, stdev=1):
        self.mean = mean
        self.stdev = stdev

    def pull_lever(self):
        reward = np.random.normal(self.mean, self.stdev)
        return np.round(reward,1)

slotA = GaussianBandit(5, 3)
slotB = GaussianBandit(6, 2)
slotC = GaussianBandit(1, 5)

game = GaussianBanditGame([slotA, slotB, slotC])
game.user_play()
\end{lstlisting}


\subsection{Action-value Methods}

One natural way to estimate this is by averaging the rewards actually received: $Q_t(a)$ is sum of rewards when $a$ taken prior $t$ over number of times $a$ taken prior to $t$:
\begin{align*}
	Q_t(a) = \frac{\sum_{t=1}^{t-1}R_i \cdot \mathds{1}_{A_i=a}}{\sum_{t=1}^{t-1}\mathds{1}_{A_i=a}}
\end{align*}

In other words, the average reward (action value) observed before the $n$-th selection of this action is $Q_n = (R_1+\dots+R_{n-1})/(n-1)$

Then, the simplest action selection rule is to select one of the actions with the highest estimated value, which is a greedy action selection method represented as follows:
$$A_t=\argmax_{a}Q_t(a).$$
This approach always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, instead select randomly from among all the actions. This near greedy action selection rule is called $\epsilon$-greedy method. In the limit as the number of steps increases, every action will be sampled an infinite number of times, which ensures all the $Q_t(a)$ converge to $q_*(a)$.

\subsection{Incremental Implementation of The Action-value Methods}
\begin{align*}
	Q_{n+1} &= \frac{1}{n}\sum_{i=1}^{n}R_i\\
	&= \frac{1}{n} \Bigg(R_n + \sum_{i=1}^{n-1}R_i \Bigg)\\
	&= \frac{1}{n} \Bigg(R_n + (n-1)\frac{1}{n-1} \sum_{i=1}^{n-1}R_i \Bigg)\\
	&= \frac{1}{n} \Bigg(R_n + (n-1)Q_n \Bigg)\\
	&=  Q_n + \frac{1}{n} \Big[R_n - Q_n \Big]
\end{align*}
\begin{itemize}
	\item $Q_n$: Old estimate
	\item $Q_{n+1}$: New estimate
	\item $R_{n}$: New reward
\end{itemize}

This is an incremental formulas for updating averages with small, constant computation required to process each new reward. This update rule can be expressed in a general form:
$$NewEstimate \leftarrow OldEstimate + StepSize \underbrace{\Big[Target - OldEstimate \Big]}_{error}.$$
The target is presumed to indicate a desirable direction in which to move, though it may be noisy. So, we adjust our current estimate, $Q_n$, in the direction of the error that we calculate based on the latest observed reward, $R_n-Q_n$, with a step size of $1/n$   and obtain  a new estimate, $Q_{n+1}$ . 

\begin{lstlisting}[language=Python]
def incremental_average(rewards):
    """
    Given a list of rewards, compute the incremental average Q_n
    after each new reward using:
    
        Q_{n+1} = Q_n + (1/n) * (R_n - Q_n)
    
    Args:
        rewards (list of float): A list of reward values R_i.

    Returns:
        list of float: The running averages Q_1, Q_2, ..., Q_n.
    """
    Q_values = []
    Q_current = 0.0  # Initial estimate (can be set to zero or any prior)
    
    for i, R in enumerate(rewards, start=1):
        # i is the current step (n), R is R_n
        # Update Q_{n+1} using the incremental formula
        Q_current = Q_current + (1.0 / i) * (R - Q_current)
        
        # Store the updated estimate
        Q_values.append(Q_current)
    
    return Q_values

# Example usage:
rewards_example = [10, 20, 15, 5, 30]
running_estimates = incremental_average(rewards_example)

for step, (reward, Q) in enumerate(zip(rewards_example, running_estimates), start=1):
    print(f"Step {step}, Reward = {reward}, Running Average = {Q:.2f}")
\end{lstlisting}







