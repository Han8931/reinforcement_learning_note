\section{Multi-Armed Bandits}

Bandit problems are stateless. Each arm has a fixed distribution of rewards. Bandit is another name of a slot machine. It does not depend on which arms were pulled previously. The goal is to explore the reward distributions of all arms and then keep pulling the best one. We only have a single chance of selecting an action in each episode. 

% Markov decision processes are a temporal extension of bandit problems: pulling an arm influences the future rewards. Technically, there is a state that changes by pulling an arm. The reward distributions depend on that state.

You can view the bandit problems as Markov decision processes where all states are terminal. In that case, all decision sequences have a length of 1 and subsequent pulls don't influence each other.

In sum,
\begin{itemize}
	\item When the lever of a slot machine is pulled it gives a random reward coming from a probability distribution specific to that machine.
	\item Although the machines look identical, their reward probability distributions are different. 
	\item In each turn, gamblers need to decide whether to play the machine that has given the highest average reward so far, or to try another machine. 
\end{itemize}

In our $k$-armed bandit problem (or \textit{multi-armed bandit problem}), each of the $k$ actions has an expected or mean reward given that that action is selected: let us call this the \textit{value} of that action. We denote the action selected on time step $t$ as $A_t$, and the corresponding reward as $R_t$. The value then of an arbitrary action $a$, denoted $q_*(a)$, is the expected reward given that $a$ is selected:
$$q_*(a) = \mathbb{E}[R_t|A_t=a].$$
Since we do not know which action is the best, we have to estimate the value of actions $a$ at time step $t$, $Q_t(a)$.

Let's say there are two slot machines with distributions as follows:
\begin{table}[h]
\begin{minipage}{0.5\textwidth}
\centering
\begin{tabular}{lllll}
\toprule
\# Coins & 0   & 1   & 2   & 3  \\
\midrule
Prob     & 0.3 & 0.2 & 0.1 & 0.4\\
\bottomrule
\end{tabular}
\caption{Slot Machine 1}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\centering
\begin{tabular}{lllll}
\toprule
\# Coins & 0   & 1   & 2   & 3  \\
\midrule
Prob     & 0.2 & 0.3 & 0.4 & 0.1\\
\bottomrule
\end{tabular}
\caption{Slot Machine 2}
\end{minipage}
\end{table}
Then, we can compute their expected value as follows:
\begin{align*}
	\mathbb{E}[S_1]&= 0*0.3+1*0.2+2*0.1+3*0.4 = 1.6 \\
	\mathbb{E}[S_2]&= 0*0.2+1*0.3+2*0.4+3*0.1 = 1.4
\end{align*}
Thus, we can say that the average number of coins we can get from the slot machine 1 is higher than that of the slot machine 2. This tells us that we should chose the slot machine 1 to maximize our rewards. 

However, we have no access to the true expected value of the slot machines, so we have to estimate them heuristically. 


% \begin{lstlisting}[language=Python]
% class GaussianBandit:
%     def __init__(self, mean=0, stdev=1):
%         self.mean = mean
%         self.stdev = stdev

%     def pull_lever(self):
%         reward = np.random.normal(self.mean, self.stdev)
%         return np.round(reward,1)

% slotA = GaussianBandit(5, 3)
% slotB = GaussianBandit(6, 2)
% slotC = GaussianBandit(1, 5)

% game = GaussianBanditGame([slotA, slotB, slotC])
% game.user_play()
% \end{lstlisting}


\subsection{Action-value Methods}

One natural way to estimate this is by averaging the rewards actually received: $Q_t(a)$ is sum of rewards when $a$ taken prior $t$ over number of times $a$ taken prior to $t$:
\begin{align*}
	Q_t(a) = \frac{\sum_{t=1}^{t-1}R_i \cdot \mathds{1}_{A_i=a}}{\sum_{t=1}^{t-1}\mathds{1}_{A_i=a}}
\end{align*}

In other words, the average reward (action value) observed before the $n$-th selection of this action is $Q_n = (R_1+\dots+R_{n-1})/(n-1)$. Note that the $Q$ stands for the quality of the action. 

Then, the simplest action selection rule is to select one of the actions with the highest estimated value, which is a \textit{greedy action selection} method represented as follows:
$$A_t=\argmax_{a}Q_t(a).$$
This approach always exploits current knowledge to maximize immediate reward (\ie \textit{exploitation}); it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, instead select randomly from among all the actions (\ie \textit{exploration}). This near greedy action selection rule is called $\epsilon$-greedy method. In the limit as the number of steps increases, every action will be sampled an infinite number of times, which ensures all the $Q_t(a)$ converge to $q_*(a)$.

\subsection{Incremental Implementation of The Action-value Methods}
\begin{align*}
	Q_{n+1} &= \frac{1}{n}\sum_{i=1}^{n}R_i\\
	&= \frac{1}{n} \Bigg(R_n + \sum_{i=1}^{n-1}R_i \Bigg)\\
	&= \frac{1}{n} \Bigg(R_n + (n-1)\frac{1}{n-1} \sum_{i=1}^{n-1}R_i \Bigg)\\
	&= \frac{1}{n} \Bigg(R_n + (n-1)Q_n \Bigg)\\
	&=  Q_n + \frac{1}{n} \Big[R_n - Q_n \Big]
\end{align*}
\begin{itemize}
	\item $Q_n$: Old estimate
	\item $Q_{n+1}$: New estimate
	\item $R_{n}$: New reward
\end{itemize}

This is an incremental formulas for updating averages with small, constant computation required to process each new reward. This update rule can be expressed in a general form:
$$NewEstimate \leftarrow OldEstimate + StepSize \underbrace{\Big[Target - OldEstimate \Big]}_{error}.$$
The target is presumed to indicate a desirable direction in which to move, though it may be noisy. So, we adjust our current estimate, $Q_n$, in the direction of the error that we calculate based on the latest observed reward, $R_n-Q_n$, with a step size of $1/n$   and obtain  a new estimate, $Q_{n+1}$ . 

\begin{lstlisting}[language=Python]
def incremental_average(rewards):
    """
    Given a list of rewards, compute the incremental average Q_n
    after each new reward using:
    
        Q_{n+1} = Q_n + (1/n) * (R_n - Q_n)
    
    Args:
        rewards (list of float): A list of reward values R_i.

    Returns:
        list of float: The running averages Q_1, Q_2, ..., Q_n.
    """
    Q_values = []
    Q_current = 0.0  # Initial estimate (can be set to zero or any prior)
    
    for i, R in enumerate(rewards, start=1):
        # i is the current step (n), R is R_n
        # Update Q_{n+1} using the incremental formula
        Q_current = Q_current + (1.0 / i) * (R - Q_current)
        
        # Store the updated estimate
        Q_values.append(Q_current)
    
    return Q_values

# Example usage:
rewards_example = [10, 20, 15, 5, 30]
running_estimates = incremental_average(rewards_example)

for step, (reward, Q) in enumerate(zip(rewards_example, running_estimates), start=1):
    print(f"Step {step}, Reward = {reward}, Running Average = {Q:.2f}")
\end{lstlisting}

We can implement the multi-armed bandit problems with the exploration as follows:

\begin{lstlisting}[language=Python]
class Bandit:
    def __init__(self, arms=10):
        self.rates = np.random.rand(arms) # Stationary distribution

    def play(self, arm):
        rate = self.rates[arm]
        if rate > np.random.rand():
            return 1
        else:
            return 0


class Agent:
    def __init__(self, epsilon, action_size=10):
        self.epsilon = epsilon
        self.Qs = np.zeros(action_size) # Q for each slot machine
        self.ns = np.zeros(action_size) # # Trials for each machine

    def update(self, action, reward):
        self.ns[action] += 1
        self.Qs[action] += (reward - self.Qs[action]) / self.ns[action] # Incremental approach

    def get_action(self):
        if np.random.rand() < self.epsilon:
            return np.random.randint(0, len(self.Qs))
        return np.argmax(self.Qs)

steps = 1000
epsilon = 0.1

bandit = Bandit()
agent = Agent(epsilon)
total_reward = 0
total_rewards = []
rates = []

for step in range(steps):
    action = agent.get_action()
    reward = bandit.play(action)
    agent.update(action, reward)
    total_reward += reward

    total_rewards.append(total_reward)
    rates.append(total_reward / (step + 1))

print(total_reward)

plt.ylabel('Total reward')
plt.xlabel('Steps')
plt.plot(total_rewards)
plt.show()

plt.ylabel('Rates')
plt.xlabel('Steps')
plt.plot(rates)
plt.show()
\end{lstlisting}

Due to the randomness introduced by the exploration, we should estimate the average return:
\begin{lstlisting}[language=Python]
runs = 200
steps = 1000
epsilon = 0.1
all_rates = np.zeros((runs, steps))  # (2000, 1000)

for run in range(runs):
    bandit = Bandit()
    agent = Agent(epsilon)
    total_reward = 0
    rates = []

    for step in range(steps):
        action = agent.get_action()
        reward = bandit.play(action)
        agent.update(action, reward)
        total_reward += reward
        rates.append(total_reward / (step + 1))

    all_rates[run] = rates

avg_rates = np.average(all_rates, axis=0)

plt.ylabel('Rates')
plt.xlabel('Steps')
plt.plot(avg_rates)
plt.show()
\end{lstlisting}








