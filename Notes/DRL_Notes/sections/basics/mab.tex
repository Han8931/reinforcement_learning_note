\section{Multi-Armed Bandits}
The name \textit{bandit} comes from one-armed bandits, a nickname for slot machines.
\begin{itemize}
	\item When the lever of a slot machine is pulled it gives a random reward coming from a probability distribution specific to that machine.
	\item Although the machines look identical, their reward probability distributions are different. 
	\item In each turn, gamblers need to decide whether to play the machine that has given the highest average reward so far, or to try another machine. 
\end{itemize}
Now, you face a dilemma:
\begin{enumerate}
	\item Try different machines to figure out which one pays the most (exploration).
	\item Stick to the machine that has given you the most money so far (exploitation).
\end{enumerate}
As you can see, balancing \textit{exploration} and \textit{exploitation} is the core idea of the bandit problem.

Note that bandit problems are \textit{stateless}. \textbf{Each arm has a fixed distribution of rewards}. It does not depend on which arms were pulled previously. The goal is to explore the reward distributions of all arms and then keep pulling the best one. We only have a single chance of selecting an action in each episode. 

You can view the bandit problems as Markov decision processes where all states are terminal. In that case, all decision sequences have a length of 1 and subsequent pulls don't influence each other.

In our \textit{multi-armed bandit problem} (MAB) (or $k$-armed bandit problem), each of the $k$ actions has an expected or mean reward given that that action is selected: let us call this the \textit{value} of that action. We denote the action selected on time step $t$ as $A_t$, and the corresponding reward as $R_t$. The value then of an arbitrary action $a$, denoted $q_*(a)$ (\cf the $q$ stands for the quality of an action), is the expected reward given that $a$ is selected:
$$q_*(a) = \mathbb{E}[R_t|A_t=a].$$
Since we do not know which action is the best, we have to estimate the value of actions $a$ at time step $t$, $Q_t(a)$.

Let's say there are two slot machines and we know their distributions. Then, we can compute their expected values.
\begin{table}[h]
\begin{minipage}{0.5\textwidth}
\centering
\begin{tabular}{lllll}
\toprule
\# Coins & 0   & 1   & 2   & 3  \\
\midrule
Prob     & 0.3 & 0.2 & 0.1 & 0.4\\
\bottomrule
\end{tabular}
\caption{Slot Machine 1}
\end{minipage}%
\begin{minipage}{0.5\textwidth}
\centering
\begin{tabular}{lllll}
\toprule
\# Coins & 0   & 1   & 2   & 3  \\
\midrule
Prob     & 0.2 & 0.3 & 0.4 & 0.1\\
\bottomrule
\end{tabular}
\caption{Slot Machine 2}
\end{minipage}
\end{table}
\begin{align*}
	\mathbb{E}[S_1]&= 0*0.3+1*0.2+2*0.1+3*0.4 = 1.6 \\
	\mathbb{E}[S_2]&= 0*0.2+1*0.3+2*0.4+3*0.1 = 1.4
\end{align*}
We can say that the average number of coins we can get from the slot machine 1 is higher than that of the slot machine 2. This tells us that we should chose the slot machine 1 to maximize our rewards. 

However, we have no access to the true expected value of the slot machines in practice. Thus, we have to estimate them heuristically. 

\subsection{Action-value Methods}

One natural way to estimate this is by averaging the rewards actually received: $Q_t(a)$ is sum of rewards when $a$ taken prior $t$ over number of times $a$ taken prior to $t$:
\begin{align*}
	Q_t(a) = \frac{\sum_{t=1}^{t-1}R_i \cdot \mathds{1}_{A_i=a}}{\sum_{t=1}^{t-1}\mathds{1}_{A_i=a}}
\end{align*}

In other words, the average reward (action value) observed before the $n$-th selection of this action is $Q_n = (R_1+\dots+R_{n-1})/(n-1)$. % Note that the $Q$ stands for the quality of the action. 

Then, the simplest action selection rule is to select one of the actions with the highest estimated value, which is a \textit{greedy action selection} method represented as follows:
$$A_t=\argmax_{a}Q_t(a).$$
This approach always exploits current knowledge to maximize immediate reward (\ie \textit{exploitation}); it spends no time at all sampling apparently inferior actions to see if they might really be better. A simple alternative is to behave greedily most of the time, but every once in a while, instead select randomly from among all the actions (\ie \textit{exploration}). This near greedy action selection rule is called $\epsilon$-greedy method. In the limit as the number of steps increases, every action will be sampled an infinite number of times, which ensures all the $Q_t(a)$ converge to $q_*(a)$.

\subsection{Incremental Implementation of The Action-value Methods}
\begin{align*}
	Q_{n+1} &= \frac{1}{n}\sum_{i=1}^{n}R_i\\
	&= \frac{1}{n} \Bigg(R_n + \sum_{i=1}^{n-1}R_i \Bigg)\\
	&= \frac{1}{n} \Bigg(R_n + (n-1)\frac{1}{n-1} \sum_{i=1}^{n-1}R_i \Bigg)\\
	&= \frac{1}{n} \Bigg(R_n + (n-1)Q_n \Bigg)\\
	&=  Q_n + \frac{1}{n} \Big[R_n - Q_n \Big]
\end{align*}
\begin{itemize}
	\item $Q_n$: Old estimate
	\item $Q_{n+1}$: New estimate
	\item $R_{n}$: New reward
\end{itemize}

This is an incremental formulas for updating averages with small, constant computation required to process each new reward. This update rule can be expressed in a general form:
$$NewEstimate \leftarrow OldEstimate + StepSize \underbrace{\Big[Target - OldEstimate \Big]}_{error}.$$
The target is presumed to indicate a desirable direction in which to move, though it may be noisy. So, we adjust our current estimate, $Q_n$, in the direction of the error that we calculate based on the latest observed reward, $R_n-Q_n$, with a step size of $1/n$   and obtain  a new estimate, $Q_{n+1}$ . 









