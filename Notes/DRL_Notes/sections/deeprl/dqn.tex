\chapter{Deep Q-Network}
Q-learning is a popular model-free reinforcement learning algorithm that is used to find an optimal action-selection policy for a given finite Markov decision process (MDP). It's a form of temporal difference learning, where the agent learns from the consequences of its actions in the environment.

The main goal of $Q$-learning is to learn a policy, represented by the action-value function $Q(s,a)$, which estimates the expected cumulative future rewards of taking action $a$ in state $s$.


$$Q(S_t, A_t) \leftarrow Q(S_t, A_t)+ \alpha \Big[R_{t+1}+\gamma \max_a Q(S_{t+1}, a)-Q(S_t, A_t)\Big] $$





\section{A Few Considerations}

The I.I.D. assumption is violated. 

\subsection{Non-Stationary Target}
Policy changes over time

\subsection{Correlated Samples}
Samples are drawn from the same trajectory.


\section{Solutions}
\subsection{Using Target Networks}
A straightforward way to make target values more stationary is to have a separate network that we can fix for multiple steps and reserve it for calculating more stationary targets. The network with this purpose in DQN is called the target network.

\subsection{Experience Replay}

Having a replay buffer allows the agent two critical things. 
\begin{itemize}
	\item First, the training process can use a more diverse mini-batch for performing updates. 
	\item Second, the agent no longer has to fit the model to the same small mini-batch for multiple iterations. Adequately sampling a sufficiently large replay buffer yields a slow-moving target, so the agent can now sample and train on every time step with a lower risk of divergence.
\end{itemize}


\section{Double DQN}
\label{sec:double_dqn}

 Q-learning tends to overestimate action- value functions. Our DQN agent is no different; we're using the same off-policy TD target, after all, with that max operator. The crux of the problem is simple: We’re taking the max of estimated values. Estimated values are often off-center, some higher than the true values, some lower, but the bottom line is that they’re off. The problem is that we’re always taking the max of these values, so we have a preference for higher values, even if they aren't correct. Our algorithms show a positive bias, and performance suffers.

One way to better understand positive bias and how we can address it when using function approximation is by unwrapping the max operator in the target calculations. The max of a Q-function is the same as the Q-function of the argmax action.

\begin{itemize}
	\item You create two action-value functions, $Q_A$ and $Q_B$.
	\item You flip a coin to decide which action-value function to update. For example, $Q_A$ on heads, $Q_B$ on tails. 
	\item If you got a heads and thus get to update $Q_A$: You select the action index to evaluate from $Q_B$, and evaluate it using the estimate $Q_A$ predicts. Then, you proceed to update $Q_A$ as usual, and leave $Q_B$ alone. 
	\item If you got a tails and thus get to update $Q_B$, you do it the other way around: get the index from $Q_A$, and get the value estimate from $Q_B$. $Q_B$ gets updated, and $Q_A$ is left alone.
\end{itemize}

Instead of adding this overhead that’s a detriment to training speed, we can perform double learning with the other network we already have, which is the target network. However, instead of training both the online and target networks, we continue training only the online network, but use the target network to help us, in a sense, cross-validate the estimates.

We want to be cautious as to which network to use for action selection and which network to use for action evaluation. Initially, we added the target network to stabilize training by avoiding chasing a moving target. To continue on this path, we want to make sure we use the network we’re training, the online network, for answering the first question. In other words, we use the online network to find the index of the best action. Then, we use the target network to ask the second question, that is, to evaluate the previously selected action.


\section{Dueling DDQN}
% \label{sec:}
The dueling network is an improvement that applies only to the net- work architecture and not the algorithm. That is, we won’t make any changes to the algorithm, but the only modifications go into the network architecture. 
