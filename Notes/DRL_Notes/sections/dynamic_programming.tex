\chapter{Dynamic Programming}

\section{Policy Evaluation}
\begin{itemize}
	\item Prediction problem: refers to the problem of \textbf{evaluating policies} (sipmly, rating policies), of estimating value functions given a policy (learning to predict returns).
	\item Control problem: problem of \textbf{finding optimal policies}. Usually solved by the pattern of generalized policy iteration (GPI), where the competing processes of policy evaluation and policy improvement progressively move policies towards optimality.
	\item Policy evaluation: refers to algorithms that solve the prediction problem.
		\begin{itemize}
			\item Iterative policy evaluation.
				$$v_{k+1}(s)=\sum_{a}\pi(a|s)\sum_{s',r}p(s',r|s,a)[r + \gamma v_k(s')].$$ 
			\item Init $v_0(s)$ for all $s$ arbitrarily and to 0 if $s$ is terminal. 
			\item Bootstrapping: $v_1(s)\to v_2(s)\to\cdots\to v_N(s)$
		\end{itemize}
\end{itemize}

\section{Policy Improvement}
\begin{itemize}
	\item Policy improvement: algorithms that make new policies that improve on an original policy by making it greedier than the original with respect to the value function of that original policy. 
		$$\pi'(s) = \argmax_a \sum_{s',r}p(s',r|s,a)[r + \gamma v_\pi(s')].$$
\end{itemize}
