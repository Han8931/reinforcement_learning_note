\chapter{Introduction}
Reinforcement learning (RL) is learning what to do so as to maximize a numerical reward signal. The learner is not told which actions to take, but instaed must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. 

Important features of RL:

\begin{itemize}
	\item Trial-and-error search
	\item Delayed reward
	\item RL uses training information that \textit{evaluates} the actions taken rather than \textit{instructs} by giving correct actions.
		\begin{itemize}
			\item Evaluative feedback, not about best or worst.
		\end{itemize}
\end{itemize}

%These two characteristics --\textbf{} and \textbf{}-- are the two most important distinguishing features of reinforcement learning.


\section{Markov Chain}
\begin{itemize}
	\item Reachable: $i\to j$
	\item Communicate: $i\leftrightarrow j$
	\item Irreducible: $i\leftrightarrow j, \forall i,j$
	\item Absorbing state: If the only possible transition is to itself. This is also a terminal state.
	\item Transient state: A state $s$ is called a transient state, if there is another state $s'$, that is reachable from $s$, but not vice versa. 
	\item Recurrent state: A state that is not transient. 
	\item Periodic: A state is periodic if all of the paths leaving $s$ come back after some multiple steps ($k>1$). 
		\begin{itemize}
			\item Recurrent state is aperiodic if $k=1$.
		\end{itemize}
	\item Ergodicity if a Markov chain follows:
		\begin{itemize}
			\item Irredicible
			\item Recurrent
			\item Aperiodic
		\end{itemize}
	%\item Steady-state probability distribution.
\end{itemize}

%$$p_n = qP^n$$


%\subsection{Bellman Optimility Equation for $v_*$ and $q_*$}
%\begin{itemize}
%	\item $v_*(s) = \max_a q_*(s,a)$
%	\item $q_*(s,a) = \mathcal{R}_s^a + \gamma \sum_{s'\in \mathcal{S}}\mathcal{P}_{ss'}^a v_*(s')$
%	\item $v_*(s) = \max_a \mathcal{R}_s^a + \gamma \sum_{s'\in \mathcal{S}}\mathcal{P}_{ss'}^a v_*(s')$
%	\item $q_*(s,a) = \mathcal{R}_s^a + \gamma \sum_{s'\in \mathcal{S}}\mathcal{P}_{ss'}^a \max_{a'} q_*(s', a')$
%\end{itemize}

