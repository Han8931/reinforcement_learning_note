\chapter{Introduction}
\section{Introduction}

\textbf{Reinforcement Learning (RL) is the science of decision-making.} It focuses on learning to make decisions that maximize a numerical reward signal. Unlike traditional approaches, the learner is not explicitly told which actions to take but must discover the most rewarding actions through experience. In complex scenarios, actions influence not only immediate rewards but also future situations and rewards. RL differs from other machine learning paradigms in several key aspects:
\begin{itemize}
    \item RL uses training information that \textit{evaluates} actions rather than \textit{instructing} by providing the correct actions.
    \begin{itemize}
        \item The feedback is evaluative, indicating how good or bad an action was, without explicitly identifying the best or worst action.
        \item There is no explicit supervisor; learning is guided by a \textit{reward signal}.
    \end{itemize}
    \item \textbf{Delayed Rewards}: Feedback is not immediate; rewards are often received after a delay, reflecting the long-term consequences of actions.
    \item Time is crucial: RL deals with sequential decision-making and non-i.i.d. (non-independent and identically distributed) data, where past actions influence future data.
    \item Actions affect the subsequent data: For instance, a robot moving through a room will see a different view with each step it takes, directly influenced by its previous movements.
    \item The environment is initially unknown: The agent interacts with the environment and improves its policy through \textbf{trial-and-error}.
    \begin{itemize}
        \item \textbf{Planning}: When the environment is known, the agent can compute optimal policies or actions.
        \item \textbf{Exploration}: The agent gathers more information about the environment by trying new actions.
        \item \textbf{Exploitation}: The agent leverages existing knowledge to maximize rewards.
    \end{itemize}
\end{itemize}



\subsection{Sequential Decision Making}
\begin{itemize}
    \item \textbf{Goal}: Select actions that maximize the total future reward.
    \item Actions may have long-term consequences, requiring foresight and planning.
    \item Rewards may be delayed.
    \item It may be better to sacrifice immediate rewards for greater long-term gains.
\end{itemize}

\subsection{The Concept of State in a Process}

A \textit{state}, $S_t$ is a summary of all the past actions sufficient to choose future actions optimally, and an agent move through states via \textit{actions}.

% For a gentle introduction to the concept of \textit{State}, we start with an informal notion of the terms \textit{Process} and State. Informally, think of a Process as producing a sequence of random outcomes at discrete time steps that we'll index by a time variable $t = 0, 1, 2, \dots$. The random outcomes produced by a Process might be key financial/trading/business metrics one cares about, such as prices of financial derivatives or the value of a portfolio held by an investor. To understand and reason about the evolution of these random outcomes of a Process, it is beneficial to focus on the internal representation of the Process at each point in time t, that is fundamentally responsible for driving the outcomes produced by the Process. We refer to this internal representation of the Process at time $t$ as the (random) \textit{State} of the Process at time $t$ and denote it as $S_t$.

Specifically, we are interested in the probability of the next State $S_{t+1}$, given the present State $S_t$ and the past States $S_0, S_1, \dots, S_{t-1}$, \ie $P[S_{t+1}|S_t, S_{t-1},\dots, S_0]$. So to clarify, we distinguish between the internal representation (State) and the output (outcomes) of the Process. The State could be any data type-it could be something as simple as the daily closing price of a single stock, or it could be something quite elaborate like the number of shares of each publicly traded stock held by each bank in the U.S., as noted at the end of each week.

\paragraph{Fully observable environment} The agent can directly observe the entire environment state. Here, the agent’s state is identical to the environment's state, and the information state is the same:
$$O_t=S_t^a=S_t^w$$
This setting can be formally represented as a \textbf{Markov Decision Process (MDP)}.

\paragraph{Partially observable environment} The agent can only observe the environment indirectly. For example, a robot using camera vision may not know its exact location, or a trading agent might only observe current prices, not broader market trends. In this case, the agent's state differs from the environment's state, leading to a formal model known as a \textbf{Partially Observable Markov Decision Process (POMDP)}.

\subsection{RL Agent Categories}
RL agents can be classified as follows:
\begin{itemize}
    \item \textbf{Value-based}: Policy is implicit, and decisions are based on value functions.
    \item \textbf{Policy-based}: The agent directly learns a policy that maps states to actions.
    \item \textbf{Actor-Critic}: Combines value-based and policy-based methods.
    \item The set of transition and reward functions is referred to as the \textbf{model} of the environment.
    \item \textbf{Model-free}: Model-free RL algorithms learn to make decisions without explicitly modeling the environment's dynamics or transitions.
    \item \textbf{Model-based}: Model-based RL algorithms learn an explicit model of the environment, including transition dynamics and reward structure. The agent builds a model and uses it for planning by simulating possible future trajectories to optimize decisions.
\end{itemize}

\subsection{Exploration and Exploitation}
\begin{itemize}
    \item RL is akin to trial-and-error learning.
    \item The agent should discover a good policy from its experiences in the environment without losing too much reward along the way.
    \item \textbf{Exploration}: The agent explores to gather more information about the environment.
    \item \textbf{Exploitation}: The agent exploits known information to maximize reward. For example,
    \begin{enumerate}
        \item Restaurant Selection:
        \begin{itemize}
            \item \textbf{Exploitation}: Visit your favorite restaurant.
            \item \textbf{Exploration}: Try a new restaurant.
        \end{itemize}
        \item Oil Drilling:
        \begin{itemize}
            \item \textbf{Exploitation}: Drill at the best-known location.
            \item \textbf{Exploration}: Test a new location.
        \end{itemize}
    \end{enumerate}
\end{itemize}

\subsection{Supervised Learning v.s. Reinforcement Learning}

The data form of RL is in the form of states, actions, and rewards: \((s, a, r, s')\), whereas that of SL has $(x, y)$. When the problem requires a sequence of decisions rather than a single labeled prediction, RL is often more suitable. SL typically outputs a single label or value at a time (e.g., a single classification). RL optimizes a policy for long-term reward and can handle dependencies among actions across multiple time steps. Also, the agent must explore actions that are not immediately optimal to discover higher long-term rewards, while also exploiting known good actions. SL usually does not deal with exploration in the same sense, because it is based on a labeled training set where the correct labels or targets are already provided.

Moreover, many real-world tasks do not have direct or well-defined labels for each possible state–action pair. Instead, they have overall performance metrics or success criteria. RL can learn which actions lead to successful outcomes purely from a reward signal, eliminating the need for manually labeled data. 


\section{Markov Chain}
\begin{itemize}
	\item Reachable: $i\to j$
	\item Communicate: $i\leftrightarrow j$
	\item Irreducible: $i\leftrightarrow j, \forall i,j$
	\item Absorbing state: If the only possible transition is to itself. This is also a terminal state.
	\item Transient state: A state $s$ is called a transient state, if there is another state $s'$, that is reachable from $s$, but not vice versa. 
	\item Recurrent state: A state that is not transient. 
	\item Periodic: A state is periodic if all of the paths leaving $s$ come back after some multiple steps ($k>1$). 
		\begin{itemize}
			\item Recurrent state is aperiodic if $k=1$.
		\end{itemize}
	\item Ergodicity if a Markov chain follows:
		\begin{itemize}
			\item Irredicible
			\item Recurrent
			\item Aperiodic
		\end{itemize}
	%\item Steady-state probability distribution.
\end{itemize}


A \textit{Time-Homogeneous Markov Process} is a Markov Process with the additional property that $P[S_{t+1}|S_t]$ is independent of $t$.
